{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4jc4TawMbWN"
      },
      "source": [
        "## Problem 1\n",
        "\n",
        "\n",
        "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
        "\n",
        "In this homework assignment, we will use Detectron2 (Facebook) to help us to do the tasks of detection and segmentation. \n",
        "\n",
        "Detectron2 is Facebook AI Research's software system that implements state-of-the-art object detection algorithms. Here, we will go through some basic usage of detectron2, and finish the problem 1 and problem 2. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzlP4u17Motj"
      },
      "source": [
        "### Getting Started"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHOtzB6TMXXN"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.7.9 64-bit' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/jaido/AppData/Local/Programs/Python/Python37/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# First step, let's install detectron2 first!\n",
        "# install dependencies: \n",
        "%pip uninstall torch torchvision -y\n",
        "%pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 torchaudio==0.10.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "%pip install pyyaml==5.1 pycocotools>=2.0.1\n",
        "\n",
        "\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v5fDAoybMsaE"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [3], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mtorchvision\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# install detectron2: (Colab has CUDA 11.1 + torch 1.10)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39m__version__\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m1.10\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch, torchvision\n",
        "\n",
        "# install detectron2: (Colab has CUDA 11.1 + torch 1.10)\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "assert torch.__version__.startswith(\"1.10\")\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html\n",
        "# It may ask you to restart the runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz23gKiWNgCc"
      },
      "outputs": [],
      "source": [
        "# Some basic setup:\n",
        "# Setup detectro2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np \n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utils\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTM6EshXM9ja"
      },
      "source": [
        "### Run a pretrained Detectron2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HAUOhCwNA6-"
      },
      "source": [
        "We first download some image from the given URLs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-aVp4NvM8cI"
      },
      "outputs": [],
      "source": [
        "!wget http://images.cocodataset.org/val2017/000000007574.jpg -q -O input.jpg\n",
        "im_input = cv2.imread(\"./input.jpg\")\n",
        "cv2_imshow(im_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjEZNOQcN-T7"
      },
      "outputs": [],
      "source": [
        "!wget http://images.cocodataset.org/val2017/000000013923.jpg -q -O test1.jpg\n",
        "im_test1 = cv2.imread(\"./test1.jpg\")\n",
        "cv2_imshow(im_test1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQAwt2RpOO7c"
      },
      "outputs": [],
      "source": [
        "!wget http://images.cocodataset.org/val2017/000000018380.jpg -q -O test2.jpg\n",
        "im_test2 = cv2.imread(\"./test2.jpg\")\n",
        "cv2_imshow(im_test2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QfDMbVfPcl6"
      },
      "source": [
        "We can see there are multiple objects in these images: bottles, tables, chairs, people, etc. Let us see if we can detect them all by using a pre-trained model given by Detectron2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USQ9qvp3PXcH"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST= 0.5  # set threshold for this model\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "outputs = predictor(im_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7dobD0RPwty"
      },
      "source": [
        "Let's take a look at the model output. \n",
        "\n",
        "In inference mode, the builtin model outputs a `list[dict]`, one dict for each image. For the object detection task, the dict contain the following fields:\n",
        "\n",
        "*   \"instances\": Instances object with the following fields:\n",
        "    * \"pred_boxes\": Storing N boxes, one for each detected instance.\n",
        "    * \"scores\": a vector of N scores.\n",
        "    * \"pred_classes\": a vector of N labels in range [0, num_categories].\n",
        "\n",
        "For more details, please see https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBDn30I1R-3b"
      },
      "outputs": [],
      "source": [
        "print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7_NJNbsQb4r"
      },
      "outputs": [],
      "source": [
        "print(outputs[\"instances\"].pred_classes)\n",
        "print(outputs[\"instances\"].pred_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlT4iUTthw9_"
      },
      "outputs": [],
      "source": [
        "outputs_q1q2 = {'q1': [], 'q2': []}\n",
        "outputs_q1q2['q1'].append(outputs[\"instances\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s44pNQB4QsNQ"
      },
      "outputs": [],
      "source": [
        "# We can use \"Visualizer\" to draw the predictions on the image\n",
        "v = Visualizer(im_input[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVJHN3ZZPzfP"
      },
      "source": [
        "AWESOME!!! Great progress so far! We are able to detect sink, microwave, bottle and even refrigerator! At this point, we have used the pre-trained model to do the inference on the given image. There are in total 17 objects are being detected. The image is adopted from the [MS-COCO](https://cocodataset.org/#home) dataset and there are 81 classes including person, bicycle, car, etc. You may find the id-category mapping [here](https://gist.github.com/AruniRC/7b3dadd004da04c80198557db5da4bda)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rgjitp2S7__"
      },
      "source": [
        "The model we just used is `COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml`. Actually, the Detectron2 provides us more than that, you may find great amouts of models for different tasks in the given [MODEL_ZOO](https://github.com/facebookresearch/detectron2/tree/master/configs). What about we try a different model to see what its output will look like? \n",
        "\n",
        "\n",
        "* Q1 (5%): Object Detection. Use the same configuration `COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml`, with IoU threshold of 0.5 (`SCORE_THRESH_TEST=0.5`), to also run inference on the rest two images (test1.jpg & test2.jpg) and view the outputs with bounding boxes. \n",
        "\n",
        "* Q2: Object Detection. Use the `COCO-Detection/faster_rcnn_R_101_FPN_3X.yaml`, which has a ResNet-101 as the backbone, with IoU threshold of 0.5 and view the outputs of all three images with bounding boxes. By looking at the outputs, can you find the difference with the one `COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml` we used in Q1? (e.g., numbers of objects, confidence scores, ...)\n",
        "\n",
        "* Q3: Object Detection. Use the `COCO-Detection/faster_rcnn_R_101_FPN_3X.yaml` with an IoU threshold of 0.9 and view the outputs of all three images with bounding boxes.\n",
        "\n",
        "* Q4 (5%): Instance Segmentation. The models we have tried in Q1-Q3 are the Faster R-CNN models for object detection. Here, let’s try a Mask R-CNN model `COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml`, with IoU threshold of 0.5, to perform the instance segmentation and view the outputs of all three images with segmentation masks. Compare the difference of outputs between an object detection model with an instance segmentation model. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4JN7anYS_4n"
      },
      "outputs": [],
      "source": [
        "# todo: Q1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W_Up2WOeTTe"
      },
      "outputs": [],
      "source": [
        "# todo: Q2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7QLEaGZesRZ"
      },
      "outputs": [],
      "source": [
        "# todo: Q3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfogta18nRL_"
      },
      "outputs": [],
      "source": [
        "# todo: Q4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tNMe2JZY0mh"
      },
      "source": [
        "### Train Faster R-CNN on a traffic sign dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEhL9Z8DY47t"
      },
      "source": [
        "We have already used the pre-trained model on MS COCO datasets. Why not we try to train our own model ourselves? Here, we will train an existing detectron2 model on a custom dataset in a new format. \n",
        "\n",
        "You have already used the pre-trained model on MS COCO datasets. Why not try to train your own model? Here, let’s train an existing Faster R-CNN model on a custom dataset in a new format. \n",
        "\n",
        "We use the [traffic sign dataset](https://www.dropbox.com/s/d8y6uc06027fpqo/traffic_sign_data.zip?dl=1). We’ll train a traffic sign detection model from an existing model pre-trained on COC dataset, available in detectron2’s model zoo. Note that the MS COCO dataset does not have the \"traffic sign\" category, but we'll be able to recognize this new class in a few minutes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1_eR921a8dk"
      },
      "source": [
        "#### Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu6JkJ7vZY61"
      },
      "outputs": [],
      "source": [
        "# download, decompress the data\n",
        "!wget https://www.dropbox.com/s/d8y6uc06027fpqo/traffic_sign_data.zip?dl=1 -O traffic_sign_data.zip\n",
        "!unzip -q traffic_sign_data.zip > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRv5xfr_Z1i8"
      },
      "source": [
        "Here, the traffic sign dataset is in its custom dataset, therefore we write a function to parse it and prepare it into detectron2's standard format. See `get_traffic_sign_dicts` function for more details. To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uoba0l8kZvnc"
      },
      "outputs": [],
      "source": [
        "from detectron2.structures import BoxMode\n",
        "\n",
        "def get_traffic_sign_dicts(data_root, txt_file):\n",
        "    dataset_dicts = []\n",
        "    filenames = []\n",
        "    csv_path = os.path.join(data_root, txt_file)\n",
        "    with open(csv_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            filenames.append(line.rstrip())\n",
        "    \n",
        "    for idx, filename in enumerate(filenames):\n",
        "        record = {}\n",
        "\n",
        "        image_path = os.path.join(data_root, filename)\n",
        "\n",
        "        height, width = cv2.imread(image_path).shape[:2]\n",
        "\n",
        "        record['file_name'] = image_path\n",
        "        record['image_id'] = idx\n",
        "        record['height'] = height\n",
        "        record['width'] = width\n",
        "\n",
        "        image_filename = os.path.basename(filename)\n",
        "        image_name = os.path.splitext(image_filename)[0]\n",
        "        annotation_path = os.path.join(data_root, 'labels', '{}.txt'.format(image_name))\n",
        "        annotation_rows = []\n",
        "\n",
        "        with open(annotation_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                temp = line.rstrip().split(\" \")\n",
        "                annotation_rows.append(temp)\n",
        "\n",
        "        objs = []\n",
        "        for row in annotation_rows:\n",
        "            xcentre = int(float(row[1])*width)\n",
        "            ycentre = int(float(row[2])*height)\n",
        "            bwidth = int(float(row[3])*width)\n",
        "            bheight = int(float(row[4])*height)\n",
        "\n",
        "            xmin = int(xcentre - bwidth/2)\n",
        "            ymin = int(ycentre - bheight/2)\n",
        "            xmax = xmin  + bwidth\n",
        "            ymax = ymin + bheight\n",
        "\n",
        "            obj= {\n",
        "                'bbox': [xmin, ymin, xmax, ymax],\n",
        "                'bbox_mode': BoxMode.XYXY_ABS,\n",
        "                # alternatively, we can use bbox_mode = BoxMode.XYWH_ABS\n",
        "                # 'bbox': [xmin, ymin, bwidth, bheight],\n",
        "                # 'bbox_mode': BoxMode.XYWH_ABS,\n",
        "                'category_id': int(row[0]),\n",
        "                'iscrowd': 0\n",
        "            }\n",
        "\n",
        "            objs.append(obj)\n",
        "        record['annotations'] = objs\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OduDa5DzaAoc"
      },
      "outputs": [],
      "source": [
        "# Metadata configurations\n",
        "data_root = \"traffic_sign_data\"\n",
        "train_txt = \"traffic_sign_train.txt\"\n",
        "test_txt = \"traffic_sign_test.txt\"\n",
        "\n",
        "train_data_name = \"traffic_sign_train\"\n",
        "test_data_name = \"traffic_sign_test\"\n",
        "\n",
        "thing_classes = [\"traffic-sign\"]\n",
        "\n",
        "output_dir = \"./outputs\"\n",
        "\n",
        "def count_lines(fname):\n",
        "    with open(fname) as f:\n",
        "        for i, l in enumerate(f):\n",
        "            pass\n",
        "    return i + 1\n",
        "\n",
        "train_img_count = count_lines(os.path.join(data_root, train_txt))\n",
        "print(\"There are {} samples in training data\".format(train_img_count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXafZZ-caKLj"
      },
      "outputs": [],
      "source": [
        "# Register the traffic_sign_train datasets\n",
        "DatasetCatalog.register(name=train_data_name, \n",
        "                        func=lambda: get_traffic_sign_dicts(data_root, train_txt))\n",
        "train_metadata = MetadataCatalog.get(train_data_name).set(thing_classes=thing_classes)\n",
        "\n",
        "# Register the traffic_sign_test datasets\n",
        "DatasetCatalog.register(name=test_data_name, \n",
        "                        func=lambda: get_traffic_sign_dicts(data_root, test_txt))\n",
        "test_metadata = MetadataCatalog.get(test_data_name).set(thing_classes=thing_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zve85TFtaUSE"
      },
      "source": [
        "To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFV07YMbaSjW"
      },
      "outputs": [],
      "source": [
        "train_data_dict = get_traffic_sign_dicts(data_root, train_txt)\n",
        "\n",
        "for d in random.sample(train_data_dict, 3):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6_48CXoasQi"
      },
      "source": [
        "#### Train!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4D3heJ3atBF"
      },
      "source": [
        "Now, let's fine-tune a COCO-pretrained R50-FPN Faster R-CNN model `COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml` on the traffic sign dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE12vSoPa-qx"
      },
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (train_data_name,)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\") # let's trainining initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.0001  # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(thing_classes)  # only has one class (traffic-sign)\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg.OUTPUT_DIR = output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6nFBRa-lLFO"
      },
      "outputs": [],
      "source": [
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PggriMXUbiXH"
      },
      "outputs": [],
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir outputs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a9aQ4_adgkB"
      },
      "source": [
        "### Inference & evaluation using the trained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZsKN_DKdkB0"
      },
      "source": [
        "Now let's run inference contains everything we've set previously. First, let's create a predictor using the model we just trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM3H6TlhdiHH"
      },
      "outputs": [],
      "source": [
        "# cfg alrady contains everything we've set previously. Now we changed it a little bit for inference:\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVmoQMxLdrLV"
      },
      "source": [
        "Then, we randomly select several samples to visualize the prediction results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYSgBxuJdoro"
      },
      "outputs": [],
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "\n",
        "test_data_dict = get_traffic_sign_dicts(data_root, test_txt)\n",
        "\n",
        "for d in random.sample(test_data_dict, 3):\n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im) \n",
        "    # print(outputs)\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=test_metadata,\n",
        "                   scale=0.5,\n",
        "                   )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYOfSMJ3gWOV"
      },
      "source": [
        "We can also evaluate its performance using AP metric implemented in COCO API. For more details about AP, please refer to [Blog](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAsXDIxmgap6"
      },
      "outputs": [],
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "\n",
        "# create evaluator instance with coco evaluator \n",
        "evaluator = COCOEvaluator(test_data_name, cfg, False, output_dir=\"./outputs/\")\n",
        "\n",
        "# create validation data loader\n",
        "val_loader = build_detection_test_loader(cfg, test_data_name)\n",
        "\n",
        "# start validation\n",
        "print(inference_on_dataset(trainer.model, val_loader, evaluator))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpLDst_9rX7Y"
      },
      "source": [
        "The AP is ~30%. You may also see the detailed metrics for small, medium and large objects as well. Not bad! Here are something that I want you to try by yourself:\n",
        "\n",
        "* Q5: Change the initial learning rate (`BASE_LR`) from `0.001` to `0.00025` and show the 4 training curves from the TensorBoard. By viewing the results (You may keep the rest of configurations fixed), does it improve the AP or not? Explain why.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj1gFvJntWE5"
      },
      "outputs": [],
      "source": [
        "# todo: Q5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnsfU0per23S"
      },
      "source": [
        "* Q6: Change the number of iterations (`MAX_ITERS`) from `300` to `500` and show the 4 training curves from the Tensorboard. By viewing the results (You may keep the rest of configurations fixed), does it improve the AP or not? What about `1000`? Explain why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaJkZNJeFLgZ"
      },
      "outputs": [],
      "source": [
        "# todo: Q6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hRCcTV-qASx"
      },
      "source": [
        "## Problem 2: Tracktor for Pedestrian Multi-Object Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoUO_XNu8SLI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "import os\n",
        "import importlib\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L35jJu9OYEc-"
      },
      "outputs": [],
      "source": [
        "# Prepare the codes\n",
        "!git clone https://github.com/phil-bergmann/tracking_wo_bnw\n",
        "\n",
        "# install packages\n",
        "!pip install -r tracking_wo_bnw/requirements.txt\n",
        "\n",
        "# install tracktor\n",
        "!pip install -e tracking_wo_bnw/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTlvpCbtZYyT"
      },
      "outputs": [],
      "source": [
        "% cd tracking_wo_bnw/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrDGOu7TYbTW"
      },
      "source": [
        "### Download MOT 17 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TWeFYhKYdKL"
      },
      "outputs": [],
      "source": [
        "# Prepare MOT-17Det datasets\n",
        "\n",
        "%%shell\n",
        "\n",
        "# download the MOT17 detection challenge\n",
        "wget https://motchallenge.net/data/MOT17Det.zip .\n",
        "# extract it in the current folder\n",
        "unzip -q MOT17Det.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn8Jj78iYhUt"
      },
      "outputs": [],
      "source": [
        "# Prepare MOT-17 label datasets\n",
        "\n",
        "%%shell\n",
        "\n",
        "wget https://motchallenge.net/data/MOT17Labels.zip .\n",
        "unzip -q -d data/MOT17Labels MOT17Labels.zip\n",
        "unzip -q -d data/MOT17Det MOT17Det.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IanQY3rbz_a"
      },
      "outputs": [],
      "source": [
        "# Obtain the ground truth and pre-trained model files\n",
        "\n",
        "%%shell\n",
        "\n",
        "wget https://vision.in.tum.de/webshare/u/meinhard/tracking_wo_bnw-output_v3.zip .\n",
        "unzip -q tracking_wo_bnw-output_v3.zip "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afjo5f2qa9ow"
      },
      "source": [
        "Let's have a look at the dataset and how it is layed down.\n",
        "\n",
        "The data is structured as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_KtxnIJYlE4"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "ls\n",
        "ls train\n",
        "ls test\n",
        "\n",
        "ls train/MOT17-02/\n",
        "ls train/MOT17-02/img1/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FR2Yo1nbB-m"
      },
      "source": [
        "Visualize some images in the MOT-17 dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hg5DdnubBVT"
      },
      "outputs": [],
      "source": [
        "# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35OgNbSGbG7B"
      },
      "source": [
        "\n",
        "### Download torchvision and coco\n",
        "\n",
        "First, we need to install `pycocotools`. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.\n",
        "\n",
        "In `references/detection/,` we have a number of helper functions to simplify training and evaluating detection models.\n",
        "Here, we will use `references/detection/engine.py`, `references/detection/utils.py` and `references/detection/transforms.py`.\n",
        "\n",
        "Let's copy those files (and their dependencies) in here so that they are available in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbURqfoRbFAF",
        "outputId": "33c17052-3e9f-4334-82c0-829c94130cd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91UYHtZebJza"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "# Install pycocotools\n",
        "git clone https://github.com/cocodataset/cocoapi.git\n",
        "cd cocoapi/PythonAPI\n",
        "python setup.py build_ext install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltXO0msSbL4h"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPKb_LUfbRFo"
      },
      "source": [
        "### Defining the Dataset\n",
        "\n",
        "The [torchvision reference scripts for training object detection, instance segmentation and person keypoint detection](https://github.com/pytorch/vision/tree/v0.3.0/references/detection) allows for easily supporting adding new custom datasets.\n",
        "The dataset should inherit from the standard `torch.utils.data.Dataset` class, and implement `__len__` and `__getitem__`.\n",
        "\n",
        "The only specificity that we require is that the dataset `__getitem__` should return:\n",
        "\n",
        "* image: a PIL Image of size (H, W)\n",
        "* target: a dict containing the following fields\n",
        "    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n",
        "    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n",
        "    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
        "    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
        "    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation.\n",
        "    * (optionally) `masks` (`UInt8Tensor[N, H, W]`): The segmentation masks for each one of the objects\n",
        "    * (optionally) `keypoints` (`FloatTensor[N, K, 3]`): For each one of the `N` objects, it contains the `K` keypoints in `[x, y, visibility]` format, defining the object. `visibility=0` means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt `references/detection/transforms.py` for your new keypoint representation\n",
        "\n",
        "If your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.\n",
        "\n",
        "Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a `get_height_and_width` method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via `__getitem__` , which loads the image in memory and is slower than if a custom method is provided.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug0veJsjbTu9"
      },
      "source": [
        "So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let's write a `torch.utils.data.Dataset` class for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG3OVPO-bVdQ"
      },
      "outputs": [],
      "source": [
        "import configparser\n",
        "import csv\n",
        "import os\n",
        "import os.path as osp\n",
        "import pickle\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import scipy\n",
        "import torch\n",
        "\n",
        "\n",
        "class MOT17ObjDetect(torch.utils.data.Dataset):\n",
        "    \"\"\" Data class for the Multiple Object Tracking Dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, transforms=None, vis_threshold=0.25):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self._vis_threshold = vis_threshold\n",
        "        self._classes = ('background', 'pedestrian')\n",
        "        self._img_paths = []\n",
        "\n",
        "        for f in os.listdir(root):\n",
        "            path = os.path.join(root, f)\n",
        "            config_file = os.path.join(path, 'seqinfo.ini')\n",
        "\n",
        "            assert os.path.exists(config_file), \\\n",
        "                'Path does not exist: {}'.format(config_file)\n",
        "\n",
        "            config = configparser.ConfigParser()\n",
        "            config.read(config_file)\n",
        "            seq_len = int(config['Sequence']['seqLength'])\n",
        "            im_width = int(config['Sequence']['imWidth'])\n",
        "            im_height = int(config['Sequence']['imHeight'])\n",
        "            im_ext = config['Sequence']['imExt']\n",
        "            im_dir = config['Sequence']['imDir']\n",
        "\n",
        "            _imDir = os.path.join(path, im_dir)\n",
        "\n",
        "            for i in range(1, seq_len + 1):\n",
        "                img_path = os.path.join(_imDir, f\"{i:06d}{im_ext}\")\n",
        "                assert os.path.exists(img_path), \\\n",
        "                    'Path does not exist: {img_path}'\n",
        "                self._img_paths.append(img_path)\n",
        "\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return len(self._classes)\n",
        "\n",
        "    def _get_annotation(self, idx):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        if 'test' in self.root:\n",
        "          \n",
        "            num_objs = 0\n",
        "            boxes = torch.zeros((num_objs, 4), dtype=torch.float32)\n",
        "\n",
        "            return {'boxes': boxes,\n",
        "                'labels': torch.ones((num_objs,), dtype=torch.int64),\n",
        "                'image_id': torch.tensor([idx]),\n",
        "                'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
        "                'iscrowd': torch.zeros((num_objs,), dtype=torch.int64),\n",
        "                'visibilities': torch.zeros((num_objs), dtype=torch.float32)}\n",
        "                \n",
        "        img_path = self._img_paths[idx]\n",
        "        file_index = int(os.path.basename(img_path).split('.')[0])\n",
        "\n",
        "        gt_file = os.path.join(os.path.dirname(\n",
        "            os.path.dirname(img_path)), 'gt', 'gt.txt')\n",
        "\n",
        "        assert os.path.exists(gt_file), \\\n",
        "            'GT file does not exist: {}'.format(gt_file)\n",
        "\n",
        "        bounding_boxes = []\n",
        "\n",
        "        with open(gt_file, \"r\") as inf:\n",
        "            reader = csv.reader(inf, delimiter=',')\n",
        "            for row in reader:\n",
        "                visibility = float(row[8])\n",
        "                if int(row[0]) == file_index and int(row[6]) == 1 and int(row[7]) == 1 and visibility >= self._vis_threshold:\n",
        "                    bb = {}\n",
        "                    bb['bb_left'] = int(row[2])\n",
        "                    bb['bb_top'] = int(row[3])\n",
        "                    bb['bb_width'] = int(row[4])\n",
        "                    bb['bb_height'] = int(row[5])\n",
        "                    bb['visibility'] = float(row[8])\n",
        "\n",
        "                    bounding_boxes.append(bb)\n",
        "\n",
        "        num_objs = len(bounding_boxes)\n",
        "\n",
        "        boxes = torch.zeros((num_objs, 4), dtype=torch.float32)\n",
        "        visibilities = torch.zeros((num_objs), dtype=torch.float32)\n",
        "        \n",
        "        for i, bb in enumerate(bounding_boxes):\n",
        "            # Make pixel indexes 0-based, should already be 0-based (or not)\n",
        "            x1 = bb['bb_left'] - 1\n",
        "            y1 = bb['bb_top'] - 1\n",
        "            # This -1 accounts for the width (width of 1 x1=x2)\n",
        "            x2 = x1 + bb['bb_width'] - 1\n",
        "            y2 = y1 + bb['bb_height'] - 1\n",
        "\n",
        "            boxes[i, 0] = x1\n",
        "            boxes[i, 1] = y1\n",
        "            boxes[i, 2] = x2\n",
        "            boxes[i, 3] = y2\n",
        "            visibilities[i] = bb['visibility']\n",
        "            \n",
        "        return {'boxes': boxes,\n",
        "                'labels': torch.ones((num_objs,), dtype=torch.int64),\n",
        "                'image_id': torch.tensor([idx]),\n",
        "                'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
        "                'iscrowd': torch.zeros((num_objs,), dtype=torch.int64),\n",
        "                'visibilities': visibilities,}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        img_path = self._img_paths[idx]\n",
        "        # mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        target = self._get_annotation(idx)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._img_paths)\n",
        "    \n",
        "    def write_results_files(self, results, output_dir):\n",
        "        \"\"\"Write the detections in the format for MOT17Det sumbission\n",
        "\n",
        "        all_boxes[image] = N x 5 array of detections in (x1, y1, x2, y2, score)\n",
        "\n",
        "        Each file contains these lines:\n",
        "        <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, <x>, <y>, <z>\n",
        "\n",
        "        Files to sumbit:\n",
        "        ./MOT17-01.txt\n",
        "        ./MOT17-02.txt\n",
        "        ./MOT17-03.txt\n",
        "        ./MOT17-04.txt\n",
        "        ./MOT17-05.txt\n",
        "        ./MOT17-06.txt\n",
        "        ./MOT17-07.txt\n",
        "        ./MOT17-08.txt\n",
        "        ./MOT17-09.txt\n",
        "        ./MOT17-10.txt\n",
        "        ./MOT17-11.txt\n",
        "        ./MOT17-12.txt\n",
        "        ./MOT17-13.txt\n",
        "        ./MOT17-14.txt\n",
        "        \"\"\"\n",
        "\n",
        "        #format_str = \"{}, -1, {}, {}, {}, {}, {}, -1, -1, -1\"\n",
        "\n",
        "        files = {}\n",
        "        for image_id, res in results.items():\n",
        "            path = self._img_paths[image_id]\n",
        "            img1, name = osp.split(path)\n",
        "            # get image number out of name\n",
        "            frame = int(name.split('.')[0])\n",
        "            # smth like /train/MOT17-09-FRCNN or /train/MOT17-09\n",
        "            tmp = osp.dirname(img1)\n",
        "            # get the folder name of the sequence and split it\n",
        "            tmp = osp.basename(tmp).split('-')\n",
        "            # Now get the output name of the file\n",
        "            out = tmp[0]+'-'+tmp[1]+'.txt'\n",
        "            outfile = osp.join(output_dir, out)\n",
        "\n",
        "            # check if out in keys and create empty list if not\n",
        "            if outfile not in files.keys():\n",
        "                files[outfile] = []\n",
        "\n",
        "            for box, score in zip(res['boxes'], res['scores']):\n",
        "                x1 = box[0].item()\n",
        "                y1 = box[1].item()\n",
        "                x2 = box[2].item()\n",
        "                y2 = box[3].item()\n",
        "                files[outfile].append(\n",
        "                    [frame, -1, x1, y1, x2 - x1, y2 - y1, score.item(), -1, -1, -1])\n",
        "\n",
        "        for k, v in files.items():\n",
        "            with open(k, \"w\") as of:\n",
        "                writer = csv.writer(of, delimiter=',')\n",
        "                for d in v:\n",
        "                    writer.writerow(d)\n",
        "\n",
        "    def print_eval(self, results, ovthresh=0.5):\n",
        "        \"\"\"Evaluates the detections (not official!!)\n",
        "\n",
        "        all_boxes[cls][image] = N x 5 array of detections in (x1, y1, x2, y2, score)\n",
        "        \"\"\"\n",
        "\n",
        "        if 'test' in self.root:\n",
        "            print('No GT data available for evaluation.')\n",
        "            return\n",
        "            \n",
        "        # Lists for tp and fp in the format tp[cls][image]\n",
        "        tp = [[] for _ in range(len(self._img_paths))]\n",
        "        fp = [[] for _ in range(len(self._img_paths))]\n",
        "\n",
        "        npos = 0\n",
        "        gt = []\n",
        "        gt_found = []\n",
        "\n",
        "        for idx in range(len(self._img_paths)):\n",
        "            annotation = self._get_annotation(idx)\n",
        "            bbox = annotation['boxes'][annotation['visibilities'].gt(self._vis_threshold)]\n",
        "            found = np.zeros(bbox.shape[0])\n",
        "            gt.append(bbox.cpu().numpy())\n",
        "            gt_found.append(found)\n",
        "\n",
        "            npos += found.shape[0]\n",
        "\n",
        "        # Loop through all images\n",
        "        # for res in results:\n",
        "        for im_index, (im_gt, found) in enumerate(zip(gt, gt_found)):\n",
        "            # Loop through dets an mark TPs and FPs\n",
        "            im_det = results[im_index]['boxes'].cpu().numpy()\n",
        "            im_tp = np.zeros(len(im_det))\n",
        "            im_fp = np.zeros(len(im_det))\n",
        "            for i, d in enumerate(im_det):\n",
        "                ovmax = -np.inf\n",
        "                if im_gt.size > 0:\n",
        "                    # compute overlaps\n",
        "                    # intersection\n",
        "                    ixmin = np.maximum(im_gt[:, 0], d[0])\n",
        "                    iymin = np.maximum(im_gt[:, 1], d[1])\n",
        "                    ixmax = np.minimum(im_gt[:, 2], d[2])\n",
        "                    iymax = np.minimum(im_gt[:, 3], d[3])\n",
        "                    iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
        "                    ih = np.maximum(iymax - iymin + 1., 0.)\n",
        "                    inters = iw * ih\n",
        "\n",
        "                    # union\n",
        "                    uni = ((d[2] - d[0] + 1.) * (d[3] - d[1] + 1.) +\n",
        "                            (im_gt[:, 2] - im_gt[:, 0] + 1.) *\n",
        "                            (im_gt[:, 3] - im_gt[:, 1] + 1.) - inters)\n",
        "                    overlaps = inters / uni\n",
        "                    ovmax = np.max(overlaps)\n",
        "                    jmax = np.argmax(overlaps)\n",
        "\n",
        "                if ovmax > ovthresh:\n",
        "                    if found[jmax] == 0:\n",
        "                        im_tp[i] = 1.\n",
        "                        found[jmax] = 1.\n",
        "                    else:\n",
        "                        im_fp[i] = 1.\n",
        "                else:\n",
        "                    im_fp[i] = 1.\n",
        "\n",
        "            tp[im_index] = im_tp\n",
        "            fp[im_index] = im_fp\n",
        "\n",
        "        # Flatten out tp and fp into a numpy array\n",
        "        i = 0\n",
        "        for im in tp:\n",
        "            if type(im) != type([]):\n",
        "                i += im.shape[0]\n",
        "\n",
        "        tp_flat = np.zeros(i)\n",
        "        fp_flat = np.zeros(i)\n",
        "\n",
        "        i = 0\n",
        "        for tp_im, fp_im in zip(tp, fp):\n",
        "            if type(tp_im) != type([]):\n",
        "                s = tp_im.shape[0]\n",
        "                tp_flat[i:s+i] = tp_im\n",
        "                fp_flat[i:s+i] = fp_im\n",
        "                i += s\n",
        "\n",
        "        tp = np.cumsum(tp_flat)\n",
        "        fp = np.cumsum(fp_flat)\n",
        "        rec = tp / float(npos)\n",
        "        # avoid divide by zero in case the first detection matches a difficult\n",
        "        # ground truth (probably not needed in my code but doesn't harm if left)\n",
        "        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
        "        tmp = np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
        "\n",
        "        # correct AP calculation\n",
        "        # first append sentinel values at the end\n",
        "        mrec = np.concatenate(([0.], rec, [1.]))\n",
        "        mpre = np.concatenate(([0.], prec, [0.]))\n",
        "\n",
        "        # compute the precision envelope\n",
        "        for i in range(mpre.size - 1, 0, -1):\n",
        "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "        # to calculate area under PR curve, look for points\n",
        "        # where X axis (recall) changes value\n",
        "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\n",
        "        # and sum (\\Delta recall) * prec\n",
        "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "\n",
        "        tp, fp, prec, rec, ap = np.max(tp), np.max(fp), prec[-1], np.max(rec), ap\n",
        "        \n",
        "        print(f\"AP: {ap} Prec: {prec} Rec: {rec} TP: {tp} FP: {fp}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBnzhKsEbW0Z"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import transforms as T\n",
        "\n",
        "dataset = MOT17ObjDetect('train')\n",
        "img, target = dataset[0]\n",
        "\n",
        "def plot(img, boxes):\n",
        "  fig, ax = plt.subplots(1, dpi=96)\n",
        "\n",
        "  img = img.mul(255).permute(1, 2, 0).byte().numpy()\n",
        "  width, height, _ = img.shape\n",
        "    \n",
        "  ax.imshow(img, cmap='gray')\n",
        "  fig.set_size_inches(width / 80, height / 80)\n",
        "\n",
        "  for box in boxes:\n",
        "      rect = plt.Rectangle(\n",
        "        (box[0], box[1]),\n",
        "        box[2] - box[0],\n",
        "        box[3] - box[1],\n",
        "        fill=False,\n",
        "        linewidth=1.0)\n",
        "      ax.add_patch(rect)\n",
        "\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "img, target = T.ToTensor()(img, target)\n",
        "plot(img, target['boxes'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81UpJiqTbYjV"
      },
      "source": [
        "That's all for the dataset. Let's see how the outputs are structured for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJvml7PGbY-w"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "      \n",
        "def get_detection_model(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    model.roi_heads.nms_thresh = 0.3\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k7Rp5kcba23"
      },
      "source": [
        "DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjYXkQfAbaHP"
      },
      "outputs": [],
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiqPRZyqbc-9"
      },
      "outputs": [],
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = MOT17ObjDetect('train', get_transform(train=True))\n",
        "dataset_no_random = MOT17ObjDetect('train', get_transform(train=False))\n",
        "dataset_test = MOT17ObjDetect('test', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "data_loader_no_random = torch.utils.data.DataLoader(\n",
        "    dataset_no_random, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWLVYz1Hbfxt"
      },
      "source": [
        "INIT MODEL AND OPTIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rbe2bRPAbeiC"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_detection_model(dataset.num_classes)\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.00001,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=10,\n",
        "                                               gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o_Xiqcjbi4w"
      },
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIWcB7fobh1i"
      },
      "outputs": [],
      "source": [
        "def evaluate_and_write_result_files(model, data_loader):\n",
        "  model.eval()\n",
        "  results = {}\n",
        "  for imgs, targets in data_loader:\n",
        "    imgs = [img.to(device) for img in imgs]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds = model(imgs)\n",
        "    \n",
        "    for pred, target in zip(preds, targets):\n",
        "        results[target['image_id'].item()] = {'boxes': pred['boxes'].cpu(),\n",
        "                                              'scores': pred['scores'].cpu()}\n",
        "\n",
        "  data_loader.dataset.print_eval(results)\n",
        "  data_loader.dataset.write_results_files(results, '/content/gdrive/MyDrive/faster_rcnn_fpn_training_mot_17/resnet50/')\n",
        "  \n",
        "# evaluate_and_write_result_files(model, data_loader_test)\n",
        "# evaluate_and_write_result_files(model, data_loader_no_random)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGYN3qILbmRg"
      },
      "source": [
        "Use the provided pretrained Faster R-CNN to further train the model for 27 epochs on MOT-17 dataset. Use the sample codes to evaluate and report the accuracy of Average Precision (AP) on both train and test set. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ok7j9Vgbk2Z"
      },
      "outputs": [],
      "source": [
        "# todo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnbYGAaobo-4"
      },
      "source": [
        "Randomly select some images and visualize their detection results.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-9Bwvijoiyz"
      },
      "outputs": [],
      "source": [
        "# todo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiUi4jSDbrGJ"
      },
      "source": [
        "The Tracktor can be configured by changing the corresponding experiments/cfgs/tracktor.yaml config file. The default configurations runs Tracktor with the FPN object detector are almost same as described in the paper except the Re-identification model is turned off (**do_reid=False, load_results=True**). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls-SF8pabtvj"
      },
      "source": [
        "Run the inference experiments/scripts/test_tracktor.py using the MOT-17 train set input. The tracking results are logged in the corresponding `outputs/ ` folder. Open one of the generated results, explain what are the first six values generated in each line? (i.e., frame_id, bounding box (xywh/xyxy?), confidence, track_id, etc.). Plot the values on the corresponding images and show the video results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T93Yz3CAdF9_"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "# There are some bugs casued by the previous installation\n",
        "pip install motmetrics\n",
        "pip install sacred==0.8.0\n",
        "pip install PyYAML==5.1.2\n",
        "pip install lap==0.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XulLEJEISxM"
      },
      "outputs": [],
      "source": [
        "% cd /content\n",
        "!git clone https://github.com/KaiyangZhou/deep-person-reid.git\n",
        "\n",
        "% cd deep-person-reid\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "\n",
        "% cd /content/tracking_wo_bnw\n",
        "!wget https://motchallenge.net/data/MOT17.zip\n",
        "!unzip -q -d data MOT17.zip\n",
        "\n",
        "!wget https://vision.in.tum.de/webshare/u/meinhard/tracking_wo_bnw-output_v5.zip .\n",
        "!unzip -q tracking_wo_bnw-output_v5.zip "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLeOBvn_QpEt"
      },
      "source": [
        "do_align: True, do_reID: False (Tracktor+CMC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa0ucZ_Zbpp5"
      },
      "outputs": [],
      "source": [
        "!python experiments/scripts/test_tracktor.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qbpvw-icqeK"
      },
      "source": [
        "The results are logged in the corresponding `output` directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbCeo1TfRV5k"
      },
      "source": [
        "Question: What are the first six values generated in each line?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJW8a0tXdBkm"
      },
      "outputs": [],
      "source": [
        "# todo: plot the values on the corresponding images and show the video results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ_6Fk_FeEsH"
      },
      "source": [
        "Evaluate the performance using test_tracktor.py and report the following metrics: MOTA, MOTP, IDF1, FP. (**Hints: These metrics have already been logged in Colab outputs from the previous problems. You can just copy down here.**) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUDNU3FOeprb"
      },
      "source": [
        "Run the inference test_tracktor.py with the changed configurations in experiments/cfgs/tracktor.yaml and evaluate the performance: \n",
        "\n",
        "* tracktor/tracker:\n",
        "    * detection_person_thresh (FRCNN score threshold for detections): 0.5\n",
        "    * detection_nms_thresh (NMS threshold for detection): 0.3\n",
        "    * number_of_iterations (maximal number of iterations): 100\n",
        "    * max_features_num (How much last appearance features are to keep): 10\n",
        "    * motion_model (motion model settings, mentioned in 2.3): disabled\n",
        "\n",
        "Feel free to change at least three hyperparameters (can be from detection or tracking). Discuss how these changes may affect the tracking performance based on MOTA. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl4WW4kEXbXk"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "How do these changes affect the tracking performance based on MOTA?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgDlwzoGqIga"
      },
      "source": [
        "## Problem 3: Train Mask R-CNN on a balloon dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8mfEhNTqM_Q"
      },
      "source": [
        "In Problem 1, we use Faster R-CNN to train on the traffic sign datasets to perform object detection. With few line modifications, we can train an instance segmentation model as well. Notice that the traffic sign dataset only contains the bounding box labeling information, with no segmentation mask labeling, which is not enough to train a Mask R-CNN model. Due to this reason, we switch to another dataset: [balloon segmentation dataset](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon), which only has one class: balloon. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zufQkLrIrTyK"
      },
      "outputs": [],
      "source": [
        "# download the ballon dataset, decompress the data\n",
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n",
        "!unzip balloon_dataset.zip > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACUSurjChw8U"
      },
      "source": [
        "Write codes to load and visualize the balloon dataset in the similar manner. You need to take a careful look at the label files and construct your `get_balloon_dicts` functions to load extra poly mask information. If you load the dataset correctly, you will see training samples like the following. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z995GiX3sH_M"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "def get_balloon_dicts(img_dir):\n",
        "    \"\"\"\n",
        "    Write your codes to Load and visualize the balloon datasets\n",
        "    \"\"\"\n",
        "    # todo\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifMSINKjJDSw"
      },
      "outputs": [],
      "source": [
        "# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVuwUxHQh4T1"
      },
      "source": [
        "Fine-tune the pre-trained model `COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml` on the balloon dataset with the following configurations and show the TensorBoard Visualization. \n",
        "    * IMS_BATCH_SIZE = 2\n",
        "    * BASE_LR = 0.00025\n",
        "    * MAX_ITER = 300\n",
        "    * ROI_HEADS.BATCH_SIZE_PER_IMG = 128\n",
        "    * ROI_HEADS.NUM_CLASSES = 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kY1ohBnt8BC"
      },
      "outputs": [],
      "source": [
        "# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG3q5xq8t4j9"
      },
      "source": [
        "Use your own trained model to do the inference on testing datasets, at least plot 3 prediction results. Then, use the COCO API to report your testing Average Precision (AP). If your model is trained correctly, you will see the prediction results like the following figures. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpcKsjCpyPf0"
      },
      "outputs": [],
      "source": [
        "# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLIVvb8xyq3p"
      },
      "source": [
        "COCO API Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox1CHTYmyqXi"
      },
      "outputs": [],
      "source": [
        "# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSt7fzaUr03v"
      },
      "source": [
        "## Problem 4: 2D Human Pose Estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH7pVTJcTp-L"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "import os\n",
        "import importlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r61jPcerznI"
      },
      "source": [
        "### (a) Prepare code and dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG7D__mMu1BQ"
      },
      "source": [
        "#### Download source code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vMsx2x-SuF6"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/princeton-vl/pytorch_stacked_hourglass.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HxXSGFpu4d8"
      },
      "source": [
        "#### Download MPII dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_8xrPxaS8ky"
      },
      "outputs": [],
      "source": [
        "!wget https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2A_56MkTifM"
      },
      "outputs": [],
      "source": [
        "!tar -xvzf mpii_human_pose_v1.tar.gz -C pytorch_stacked_hourglass/data/MPII/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3H2jwILr7oo"
      },
      "source": [
        "### (b) Visualize some images in MPII dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bxUc6NWsAFv"
      },
      "outputs": [],
      "source": [
        "# todo: visualize some images in MPII dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi9dmeC7sSuA"
      },
      "source": [
        "## (c) Train the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfXw4vmLscC5"
      },
      "source": [
        "Train with 2 stack (2HG). Please keep your terminal output in order to get full credit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJPJ7DqnYGwe"
      },
      "outputs": [],
      "source": [
        "%cd pytorch_stacked_hourglass/\n",
        "!python train.py -e test_run_001 --max_iters 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F8yoJUPuIhp"
      },
      "source": [
        "Draw your 2HG loss plot from log file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTj5-KUUaQbc"
      },
      "outputs": [],
      "source": [
        "# todo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMECoBaIZBRw"
      },
      "source": [
        "Evaluate your trained models on the MPII validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cKejuY4ZA6l"
      },
      "outputs": [],
      "source": [
        "!python test.py -c test_run_001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4vQ_cMMufi0"
      },
      "source": [
        "## (d) Inference and Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wEBQ0Z3vOGa"
      },
      "source": [
        "### Infer HPE using the pretrain model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOIKvp7hm97E"
      },
      "source": [
        "Download 2HG and 8HG pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtv4b8K2m9XX"
      },
      "outputs": [],
      "source": [
        "!wget http://www-personal.umich.edu/~cnris/original_8hg/checkpoint.pt -P ./exp/test_run_003/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj1NxoxXnU11"
      },
      "outputs": [],
      "source": [
        "!wget http://www-personal.umich.edu/~cnris/original_2hg/checkpoint.pt -P ./exp/test_run_002/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C43fMYREveeg"
      },
      "source": [
        "Infer and evaluate the pretrained 2HG model. Please keep your terminal output in order to get full credit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOlgnHR5vEaF"
      },
      "outputs": [],
      "source": [
        "!python test.py -c test_run_002"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx2awjtqvlBJ"
      },
      "source": [
        "Infer and evaluate the pretrained 8HG model. Please keep your terminal output in order to get full credit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k44xxnm1vmDD"
      },
      "outputs": [],
      "source": [
        "# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0xAES_zvo5M"
      },
      "source": [
        "Visualize some human pose estimation results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtwrdB5EZLsa"
      },
      "source": [
        "2HG results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "587wRfsqv2UQ"
      },
      "outputs": [],
      "source": [
        "# todo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3WlZ2vIZPCc"
      },
      "source": [
        "8HG results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0dQsc2PZIi-"
      },
      "outputs": [],
      "source": [
        "# todo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpQNvQNhv21r"
      },
      "source": [
        "### Try customized images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24sw-SFhv_FB"
      },
      "source": [
        "Download some images from the internet and infer and visualize the human pose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWHfhaaSmQF4"
      },
      "outputs": [],
      "source": [
        "# todo"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "30f01b6890999bdb057064c622fb7bb23ad77b550cbc5eabe14fe616bf9395f3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
